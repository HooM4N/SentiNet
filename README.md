# SentiNet
![Python](https://img.shields.io/badge/Python-3.12-blue?logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-2.8-red?logo=pytorch)
![Transformers](https://img.shields.io/badge/Transformers-4.57.0-orange?logo=huggingface)
![Tokenizers](https://img.shields.io/badge/Tokenizers-0.22.1-lightgrey?logo=huggingface)
![DeBERTa v3](https://img.shields.io/badge/DeBERTa-v3-green?logo=microsoft)
![Dataset](https://img.shields.io/badge/Dataset-IMDb-red)
![Task](https://img.shields.io/badge/Task-Sentiment_Analysis-blue)
![License](https://img.shields.io/badge/License-MIT-blue)

## üß† Introduction

**SentiNet** aims to tackle the *‚ÄúHello World‚Äù* of NLP ‚Äî **IMDb movie review sentiment analysis**.
Although the dataset appears simple, it‚Äôs far from trivial. Achieving high accuracy requires robust natural language understanding to handle **sarcasm**, **sentiment flips**, and **subtle linguistic cues**.

In this project, I explored several approaches to this task:

| Approach                                | Description                                                                                                                                                                                                              |
| --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Fine-tuned Transformer (DeBERTa-v3)** | Fine-tuned **Microsoft‚Äôs DeBERTa-v3** on IMDb reviews with a custom projection head and `[CLS]` pooling for classification. Achieves strong contextual understanding and robustness to sarcasm and sentiment flips. |
| **BiGRU + Pretrained GloVe Embeddings**            | A **Bidirectional GRU** network initialized with **GloVe embeddings**, capturing sequential dependencies and semantic similarity from pretrained word vectors.                                                           |
| **BiGRU + WordPiece Tokenizer**         | Another **BiGRU** model, but trained using a **custom WordPiece tokenizer** to better handle rare and out-of-vocabulary words.                                                                                           |
| **Classic ML Baselines**                | Traditional models (Logistic Regression, Na√Øve Bayes, Gradient Boosting, Bagging) trained on TF-IDF features for comparison and benchmarking.                                                                            |

üìì **Comprehensive Jupyter Notebook**: [https://github.com/Hoom4n/SentiNet/blob/main/SentiNet.ipynb](https://github.com/Hoom4n/SentiNet/blob/main/SentiNet.ipynb)

üöÄ **Try Online Demo on Hugging Face ü§ó**: (TODO)

üóÇÔ∏è **Transformer-Based Model on Hugging Face**: (TODO)

## üìä Results

I evaluated models both quantitatively (via **F1 score**) and qualitatively on examples featuring linguistic nuances such as sarcasm, mixed sentiment, and negation.

### üß© **Quantitative Evaluation**

| Model                        | Train F1 | Val F1    | Test F1   |
| ---------------------------- | -------- | --------- | --------- |
| Logistic Regression + TF-IDF | 0.939    | 0.897     | 0.886     |
| BiGRU + WordPiece Tokenizer  | 0.928    | 0.854     | 0.850     |
| BiGRU + Pretrained GloVe Embeddings     | 0.921    | 0.881     | 0.866     |
| Transformer (DeBERTa-v3)     | ‚Äî        | **0.948** | **0.954** |



### üß© **Qualitative NLU Evaluation**

The table below summarizes how each model handled five linguistically challenging samples ‚Äî including sarcasm, shifting sentiment, and negation-based flips.
Confidence values are shown in parentheses.

| Text                                                                                 | Actual Sentiment | Challenge Type             | Logistic Reg.         | BiGRU + GloVe         | BiGRU + WordPiece     | Transformer (DeBERTa-v3) |
| ------------------------------------------------------------------------------------ | ---------------- | -------------------------- | --------------------- | --------------------- | --------------------- | ------------------------ |
| *The movie was short, simple, and absolutely wonderful.*                             | Positive         | Straightforward sentiment  | ‚úÖ **Positive (0.99)** | ‚úÖ **Positive (0.97)** | ‚úÖ **Positive (0.92)** | ‚úÖ **Positive (1.00)**    |
| *The first half was boring and predictable, but the ending completely blew me away.* | Positive         | Shifting tone (neg‚Üípos)    | ‚ùå **Negative (0.89)** | ‚ùå **Negative (0.93)** | ‚ùå **Negative (0.88)** | ‚úÖ **Positive (0.98)**    |
| *Yeah, sure, this was the ‚Äúbest‚Äù film ever... if you enjoy watching paint dry.*      | Negative         | Sarcasm / irony            | ‚ùå **Positive (0.72)** | ‚ùå **Positive (0.95)** | ‚úÖ **Negative (0.59)** | ‚úÖ **Negative (0.88)**    |
| *The acting was decent, but the script was weak and the pacing dragged.*             | Negative         | Mixed but overall negative | ‚úÖ **Negative (0.98)** | ‚úÖ **Negative (0.91)** | ‚úÖ **Negative (0.79)** | ‚úÖ **Negative (0.98)**    |
| *I didn‚Äôt expect much, yet it turned out surprisingly good.*                         | Positive         | Negation & contrast flip   | ‚úÖ **Positive (0.84)** | ‚úÖ **Positive (0.93)** | ‚úÖ **Positive (0.93)** | ‚úÖ **Positive (0.98)**    |

